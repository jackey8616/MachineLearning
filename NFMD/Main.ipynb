{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time, datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "class Printer:\n",
    "    \n",
    "    def __init__(self, logFile, initTime=time.time()):\n",
    "        self.initTime = initTime\n",
    "        self.log = self.LogInit(logFile) if logFile is not None else None\n",
    "        self.Print('Printer Inited')\n",
    "    \n",
    "    def LogInit(self, filename, mode='w+'):\n",
    "        return open(filename, mode)\n",
    "    \n",
    "    def LogClose(self,):\n",
    "        self.log.close()\n",
    "    \n",
    "    def Print(self, msg, end='\\n', timeInput=True, log=True):\n",
    "        output = ' {}'.format(msg) if timeInput == False else '[{} ({:9.3f} Sec)] {}'.format(str(datetime.datetime.now()), time.time() - self.initTime, msg)\n",
    "        print(output, end=end)\n",
    "        if log:\n",
    "            self.log.write(output + end)\n",
    "        \n",
    "    def PrintKnownPredictReport(self, true, predict):\n",
    "        accuracy = metrics.accuracy_score(true, predict)\n",
    "        self.Print('Accuracy: ', end='')\n",
    "        self.Print(accuracy, timeInput=False)\n",
    "        accuracyNor = metrics.accuracy_score(true, predict, normalize=False)\n",
    "        self.Print('Accuracy(Sameples):', end='')\n",
    "        self.Print(accuracyNor, timeInput=False)\n",
    "\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(true, predict)\n",
    "        self.Print('ROC curve:')\n",
    "        self.Print('-- TPR = TP / (TP + FN): ', end='')\n",
    "        self.Print(tpr, timeInput=False)\n",
    "        self.Print('-- FPR = FP / (FP + TN): ', end='')\n",
    "        self.Print(fpr, timeInput=False)\n",
    "        self.Print('-- Thresholds: ', end='')\n",
    "        self.Print(thresholds, timeInput=False)\n",
    "\n",
    "        auc = metrics.auc(fpr, tpr)\n",
    "        self.Print('AUC: ', end='')\n",
    "        self.Print(auc, timeInput=False)\n",
    "        \n",
    "    def DrawScatter(self, X, Y, Z, C, xlabel, ylabel, zlabel, figsize=(8, 6), dpi=400):\n",
    "        fig = plg.figure(figsize=figsize, dpi=dpi)\n",
    "        ax = Axes3D(fig)\n",
    "        ax.set_xlabel(xlabel)\n",
    "        ax.set_ylabel(ylabel)\n",
    "        ax.set_zlabel(zlabel)\n",
    "        ax.scatter(X, Y, Z, C)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import join, isfile, isdir\n",
    "import pandas as pd\n",
    "\n",
    "class Filer:\n",
    "    \n",
    "    def __init__(self, printer):\n",
    "        self.printer = printer\n",
    "        self.printer.Print('Filer Inited.')\n",
    "    \n",
    "    def getFiles(self, filePath):\n",
    "        files = []\n",
    "        if(isfile(filePath)):\n",
    "            return [filePath]\n",
    "        for f in listdir(filePath):\n",
    "            if isfile(join(filePath, f)):\n",
    "                files.append(join(filePath, f))\n",
    "            elif isdir(join(filePath, f)):\n",
    "                files.extend(self.getFiles(join(filePath, f)))\n",
    "        return files\n",
    "    \n",
    "    def concatFiles(self, filesPath, dtype=None, dropDupSet=None):\n",
    "        self.printer.Print('Concating files from path: %s' % filesPath)\n",
    "        if dropDupSet is not None:\n",
    "            self.printer.Print('Rows will be droped base on duplicate field check: %s' % str(dropDupSet))\n",
    "        folder = filesPath\n",
    "        files = self.getFiles(filesPath)\n",
    "        colDataNames = ['FileID', 'CustomerID', 'QueryTS', 'ProductID']\n",
    "        colVirNames = ['FileID', 'VirusRate']\n",
    "        count = 0\n",
    "        \n",
    "        _data = pd.DataFrame()\n",
    "        for filename in files:\n",
    "            self.printer.Print('- (File %d) %s concating...' %  (count + 1, filename))\n",
    "            _read = pd.read_csv(filename, names=colDataNames, dtype=dtype)\n",
    "            _data = pd.concat([_data, _read], axis=0)\n",
    "            if dropDupSet is not None:\n",
    "                beforeDropLen = len(_data)\n",
    "                _data = _data.drop_duplicates(subset=dropDupSet)\n",
    "                self.printer.Print('-- After concate, original length: %d, after drop length: %s.' % (beforeDropLen, len(_data)))\n",
    "            del _read\n",
    "            self.printer.Print('- %s done.' % filename)\n",
    "            count += 1\n",
    "        self.printer.Print('Files concated.')\n",
    "        return _data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-02-08 04:51:32.953449 ( 4372.207 Sec)] Printer Inited\n",
      "[2018-02-08 04:51:32.953449 ( 4372.207 Sec)] Filer Inited.\n",
      "[2018-02-08 04:51:32.969074 ( 4372.222 Sec)] Concating files from path: ./data/exam/\n",
      "[2018-02-08 04:51:32.969074 ( 4372.222 Sec)] Rows will be droped base on duplicate field check: ['FileID', 'CustomerID', 'ProductID']\n",
      "[2018-02-08 04:51:32.969074 ( 4372.222 Sec)] - (File 1) ./data/exam/0301-Copy1.csv concating...\n",
      "[2018-02-08 04:51:33.656487 ( 4372.910 Sec)] -- After concate, original length: 475569, after drop length: 207224.\n",
      "[2018-02-08 04:51:33.656487 ( 4372.910 Sec)] - ./data/exam/0301-Copy1.csv done.\n",
      "[2018-02-08 04:51:33.656487 ( 4372.910 Sec)] - (File 2) ./data/exam/0302-Copy1.csv concating...\n",
      "[2018-02-08 04:51:34.484622 ( 4373.738 Sec)] -- After concate, original length: 640906, after drop length: 432224.\n",
      "[2018-02-08 04:51:34.484622 ( 4373.738 Sec)] - ./data/exam/0302-Copy1.csv done.\n",
      "[2018-02-08 04:51:34.484622 ( 4373.738 Sec)] - (File 3) ./data/exam/0303-Copy1.csv concating...\n",
      "[2018-02-08 04:51:35.403639 ( 4374.657 Sec)] -- After concate, original length: 839112, after drop length: 625709.\n",
      "[2018-02-08 04:51:35.419263 ( 4374.672 Sec)] - ./data/exam/0303-Copy1.csv done.\n",
      "[2018-02-08 04:51:35.419263 ( 4374.672 Sec)] - (File 4) ./data/exam/0501-Copy1.csv concating...\n",
      "[2018-02-08 04:51:36.639662 ( 4375.893 Sec)] -- After concate, original length: 1155097, after drop length: 1015764.\n",
      "[2018-02-08 04:51:36.655295 ( 4375.908 Sec)] - ./data/exam/0501-Copy1.csv done.\n",
      "[2018-02-08 04:51:36.655295 ( 4375.908 Sec)] - (File 5) ./data/exam/0502-Copy1.csv concating...\n",
      "[2018-02-08 04:51:37.983413 ( 4377.237 Sec)] -- After concate, original length: 1473941, after drop length: 1299369.\n",
      "[2018-02-08 04:51:37.983413 ( 4377.237 Sec)] - ./data/exam/0502-Copy1.csv done.\n",
      "[2018-02-08 04:51:37.983413 ( 4377.237 Sec)] - (File 6) ./data/exam/0503-Copy1.csv concating...\n",
      "[2018-02-08 04:51:39.653859 ( 4378.907 Sec)] -- After concate, original length: 1832972, after drop length: 1651503.\n",
      "[2018-02-08 04:51:39.669482 ( 4378.923 Sec)] - ./data/exam/0503-Copy1.csv done.\n",
      "[2018-02-08 04:51:39.669482 ( 4378.923 Sec)] Files concated.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "\n",
    "modelFile = './model_saves/Sklearn/SkLogisticReg.pkl'\n",
    "logFile = './logs/SkLogisticReg_log_%s.log' % str(time.time())\n",
    "printer = Printer(logFile)\n",
    "filer = Filer(printer=printer)\n",
    "\n",
    "#_data = filer.concatFiles('./data/train_data.csv', dropDupSet=['FileID', 'CustomerID', 'ProductID']) # Read whole data\n",
    "#_data = filer.concatFiles('./data/train_data/_03/0301.csv', dtype={'FileID': str, 'CustomerID':  str, 'QueryTS': int, 'ProductID': str}, dropDupSet=['FileID', 'CustomerID', 'ProductID']) # Read whole data\n",
    "_data = filer.concatFiles('./data/exam/', dtype={'FileID': str, 'CustomerID':  str, 'QueryTS': int, 'ProductID': str}, dropDupSet=['FileID', 'CustomerID', 'ProductID']) # Read whole data\n",
    "_train_set = pd.read_csv('./data/training-set.csv', names=['FileID', 'VirusRate'], dtype={'FileID': str, 'VirusRate': float}) # Read sets VirusRate filed file.\n",
    "excTrain = pd.read_csv('./data/exception/exception_train.txt', names=['FileID'], dtype={'FileID': str}) # Read excepted data\n",
    "_data = pd.merge(_data, _train_set, how='left', on='FileID') # Merge with set.csv VirusRate field\n",
    "df = _data.loc[pd.merge(_data, excTrain, how='left', on='FileID', indicator=True)['_merge'] == 'left_only'] # Remove excepted data\n",
    "\n",
    "df.VirusRate = df.VirusRate.fillna(float(0.5))\n",
    "df.CustomerID = LabelEncoder().fit_transform(df.CustomerID) # CustomerID - label transform\n",
    "#df.QueryTS = MinMaxScaler().fit_transform(df.QueryTS.values.reshape(len(df.QueryTS), 1)) # QueryTS - MinMaxScale to 0 - 1 \n",
    "df.ProductID = LabelEncoder().fit_transform(df.ProductID)\n",
    "#dummyProductID = pd.get_dummies(df.ProductID)\n",
    "#dummyProductID = dummyProductID.rename(columns=lambda x: 'ProductID_' + x)\n",
    "#df = df.drop('ProductID', axis=1)\n",
    "#df = df.join(dummyProductID) # ProductID - OneHot encode\n",
    "\n",
    "#unknownRate = df[df.VirusRate == float(0.5)]\n",
    "#knownRate = df[df.VirusRate != float(0.5)]\n",
    "#knownRateSample = knownRate.sample(frac=0.2)\n",
    "train_X = df.drop(labels=['FileID', 'VirusRate'], axis=1)\n",
    "train_y = df.VirusRate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-02-08 04:51:47.609174 ( 4386.862 Sec)] Printer Inited\n",
      "[2018-02-08 04:51:47.609174 ( 4386.862 Sec)] Filer Inited.\n",
      "[2018-02-08 04:51:47.609174 ( 4386.862 Sec)] -- Model inited.\n",
      "[2018-02-08 04:51:47.609174 ( 4386.862 Sec)] -- Training...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: 'continuous'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-88-89d757937cbb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mprinter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPrint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-- Model inited.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mprinter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPrint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-- Training...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mlogistic_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogistic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'FileID'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'VirusRate'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogistic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodelFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mprinter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPrint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-- Model saved.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1215\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype,\n\u001b[0;32m   1216\u001b[0m                          order=\"C\")\n\u001b[1;32m-> 1217\u001b[1;33m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1218\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\multiclass.py\u001b[0m in \u001b[0;36mcheck_classification_targets\u001b[1;34m(y)\u001b[0m\n\u001b[0;32m    170\u001b[0m     if y_type not in ['binary', 'multiclass', 'multiclass-multioutput',\n\u001b[0;32m    171\u001b[0m                       'multilabel-indicator', 'multilabel-sequences']:\n\u001b[1;32m--> 172\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Unknown label type: %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown label type: 'continuous'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from sklearn import linear_model, metrics\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "modelFile = './model_saves/Sklearn/SkLogisticReg_%s.pkl' % str(time.time())\n",
    "logFile = './logs/SkLogisticReg_log_%s.log' % str(time.time())\n",
    "printer = Printer(logFile)\n",
    "filer = Filer(printer=printer)\n",
    "\n",
    "logistic = linear_model.LogisticRegression()\n",
    "logistic = joblib.load(modelFile) if isfile(modelFile) else logistic\n",
    "printer.Print('-- Model inited.')\n",
    "printer.Print('-- Training...')\n",
    "logistic_model = logistic.fit(df.drop(labels=['FileID', 'VirusRate'], axis=1), train_y)\n",
    "joblib.dump(logistic, modelFile)\n",
    "printer.Print('-- Model saved.')\n",
    "\n",
    "nowRate = df[df.VirusRate != float(0.5)]\n",
    "pred = logistic_model.predict(nowRate)\n",
    "prob = logistic_model.predict_proba(nowRate)\n",
    "dfVirusRate = pd.DataFrame({'VirusRate': prob[:,:1].flatten()})\n",
    "whole = pd.merge(unknownRate, dfVirusRate, left_index=True, right_index=True)\n",
    "\n",
    "printer.Print('-- Coef: ', end='')\n",
    "printer.Print(logistic_model.coef_, timeInput=False)\n",
    "printer.Print('-- Intercept: ', end='')\n",
    "printer.Print(logistic_model.intercept_, timeInput=False)\n",
    "printer.Print('-- Predict: ', end='')\n",
    "printer.Print(test_y_pre, timeInput=False)\n",
    "printer.Print('-- Proba: ', end='')\n",
    "printer.Print(test_y_pro, timeInput=False)\n",
    "printer.PrintKnownPredictReport(nowRate.VirusRate, test_y_pre)\n",
    "\n",
    "#plot_decision_regions(knownRate.drop(labels=['FileID', 'VirusRate'], axis=1), knownRate.VirusRate, clf=logistic)\n",
    "#plt.show()\n",
    "\n",
    "printer.Print('[-- Done --]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<class 'pandas.core.frame.DataFrame'>\n",
    "Int64Index: 61942383 entries, 0 to 61942382\n",
    "Data columns (total 5 columns):\n",
    "FileID        object\n",
    "CustomerID    object\n",
    "QueryTS       int64\n",
    "ProductID     object\n",
    "VirusRate     float64\n",
    "dtypes: float64(1), int64(1), object(3)\n",
    "memory usage: 15.3 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing, linear_model, metrics\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.utils import multiclass\n",
    "\n",
    "trainBool = False\n",
    "testBool = True\n",
    "modelFile = './model_saves/Sklearn/SkLogisticReg.pkl'\n",
    "logFile = './logs/SkLogisticReg_log_%s.log' % str(time.time())\n",
    "printer = Printer(logFile)\n",
    "filer = Filer(printer=printer)\n",
    "\n",
    "if trainBool:\n",
    "    _data03 = filer.concatFiles('./data/train_data/_04/')\n",
    "    _train = pd.read_csv('./data/training-set.csv', names=['FileID', 'VirusRate'], dtype={'FileID': str, 'VirusRate': float})\n",
    "    excTrain = pd.read_csv('./data/exception/exception_train.txt', names=['FileID'])\n",
    "    _train = _train.loc[pd.merge(_train, excTrain, how='left', on='FileID', indicator=True)['_merge'] == 'left_only']\n",
    "    df = pd.merge(_data03.copy(True), _train, how='left', on='FileID')\n",
    "    df = df[df.VirusRate.notnull()]\n",
    "    dfOrigin = df.copy(True)\n",
    "\n",
    "if testBool:\n",
    "    _data04 = filer.concatFiles('./data/train_data/_05/')\n",
    "    #_test = pd.read_csv('./data/testing-set.csv', names=['FileID', 'VirusRate'], dtype={'FileID': str, 'VirusRate': float})\n",
    "    _train = pd.read_csv('./data/training-set.csv', names=['FileID', 'VirusRate'], dtype={'FileID': str, 'VirusRate': float})\n",
    "    excTrain = pd.read_csv('./data/exception/exception_train.txt', names=['FileID'])\n",
    "    _train = _train.loc[pd.merge(_train, excTrain, how='left', on='FileID', indicator=True)['_merge'] == 'left_only']\n",
    "    df2 = pd.merge(_data04.copy(True), _train, how='left', on='FileID')\n",
    "    df2.VirusRate.fillna(float(0.0), inplace=True)\n",
    "    df2Origin = df2.copy(True)\n",
    "    \n",
    "if trainBool:\n",
    "    df.CustomerID = LabelEncoder().fit_transform(df.CustomerID)\n",
    "    df.ProductID = LabelEncoder().fit_transform(df.ProductID)\n",
    "    #train_X, test_X, train_y, test_y = train_test_split(df.drop(labels=['FileID', 'VirusRate'], axis=1), df.VirusRate, test_size=0.1)\n",
    "    train_X = df.drop(labels=['FileID', 'VirusRate'], axis=1)\n",
    "    train_y = df.VirusRate\n",
    "\n",
    "if testBool:\n",
    "    df2.CustomerID = LabelEncoder().fit_transform(df2.CustomerID)\n",
    "    df2.ProductID = LabelEncoder().fit_transform(df2.ProductID)\n",
    "    test_X = df2.drop(labels=['FileID', 'VirusRate'], axis=1)\n",
    "    test_y = df2.VirusRate\n",
    "\n",
    "#logistic = linear_model.LogisticRegression(multi_class='multinomial', solver='newton-cg')\n",
    "logistic = linear_model.LogisticRegression(solver='sag')\n",
    "logistic = joblib.load(modelFile) if isfile(modelFile) else logistic\n",
    "printer.Print('Model inited.')\n",
    "if trainBool:\n",
    "    printer.Print('Training...')\n",
    "    logistic_model = logistic.fit(train_X, train_y)\n",
    "    joblib.dump(logistic, modelFile)\n",
    "    printer.Print('Model saved.')\n",
    "    \n",
    "if testBool:\n",
    "    printer.Print('Predicting...')\n",
    "    logistic_model = logistic\n",
    "    test_y_predict = logistic_model.predict(test_X)\n",
    "    test_y_proba = logistic_model.predict_proba(test_X)\n",
    "    dfVirusRate = pd.DataFrame({'VirusRate': test_y_proba[:,:1].flatten()})\n",
    "    whole = pd.merge(test_X, dfVirusRate, left_index=True, right_index=True)\n",
    "    \n",
    "    printer.Print('Coef: ', end='')\n",
    "    printer.Print(logistic_model.coef_, timeInput=False)\n",
    "    printer.Print('Intercept: ', end='')\n",
    "    printer.Print(logistic_model.intercept_, timeInput=False)\n",
    "    printer.Print('Predict: ', end='')\n",
    "    printer.Print(test_y_predict, timeInput=False)\n",
    "    printer.Print('')\n",
    "    printer.Print('Whole:', timeInput=False)\n",
    "    printer.Print(whole, timeInput=False)\n",
    "\n",
    "    printer.PrintKnownPredictReport(test_y, test_y_predict)\n",
    "\n",
    "printer.Print('Done')\n",
    "printer.LogClose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
